\chapter{Implementierung}

\section{Projektstruktur}

Die Projektstruktur ist so gewählt, dass der zentrale Anwendungsteil den vollständigen Code für das Kernmodul enthält. 
Innerhalb dieses Moduls werden die verschiedenen Rollen des Systems klar voneinander getrennt umgesetzt. 
Komponenten, die nicht direkt zum Kerngeschäft gehören, wie Infrastrukturkomponenten oder Tests, 
sind in eigenständige Projekte ausgelagert. Dadurch wird die Wartbarkeit verbessert und eine klare Trennung 
der Verantwortlichkeiten erreicht. Die Gesamtstruktur des Projekts ist in Abbildung~\ref{fig:project-structure} dargestellt.

\definecolor{folderbg}{RGB}{124,166,198}
\definecolor{folderborder}{RGB}{110,144,169}

\def\Size{4pt}
\tikzset{
  folder/.pic={
    \filldraw[draw=folderborder,top color=folderbg!50,bottom color=folderbg]
      (-1.05*\Size,0.2\Size+5pt) rectangle ++(.75*\Size,-0.2\Size-5pt);  
    \filldraw[draw=folderborder,top color=folderbg!50,bottom color=folderbg]
      (-1.15*\Size,-\Size) rectangle (1.15*\Size,\Size);
  }
}

\label{fi:project-structure}
\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    inner xsep=7pt,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- (.child anchor) pic {folder} \forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=15pt},
  }  
[BachelorAkkaNet
  [FormulaOneAkkaNet
    [Config]
    [Ingress]
    [Coordinator]
    [ShardRegion]
  ]
  [Infrastructure
    [General]
    [Http]
    [PubSub]
    [ShardRegion]
  ]
  [Client]
  [Tests]
]
\end{forest}

Die modulare Struktur des Projekts stellt sicher, dass die implementierten Rollen des Systems klar voneinander 
abgegrenzt sind. Jede Modulkomponente enthält ausschließlich die für ihre Rolle relevanten Aktorimplementierungen 
und Logik. Die einzige gemeinsam genutzte Komponente ist die zentrale Konfiguration, die im Ordner 
\textit{Config} abgelegt ist. Sie stellt sicher, dass alle Module konsistent in das .NET-Hostingmodell eingebunden 
werden können.

Das Projekt \textit{Infrastructure} umfasst wiederverwendbare Funktionen und Hilfskomponenten, die sowohl von den 
Hauptmodulen als auch von den Systemtests und dem Client genutzt werden. Im \textit{Client}-Projekt erfolgt die 
Darstellung und Auswertung der erfassten Daten, wodurch eine externe Visualisierung des Systemzustands möglich wird.

Für die Implementierung wurden zentrale Bibliotheken aus dem Akka.NET-Ökosystem verwendet. 
Dazu gehören Pakete für Clusterbildung, Sharding, Persistenz und Stream-Verarbeitung. 
Zusätzlich kommen Hosting-Erweiterungen zum Einsatz, welche die Konfiguration der Cluster- 
und Persistenzkomponenten über das .NET-Hostingmodell ermöglichen. Ergänzend werden Npgsql 
für den Datenbankzugriff und Serilog für das Logging eingesetzt. Tabelle~\ref{tab:packages} 
gibt einen Überblick über die relevanten Kernpakete.


\begin{table}[H]
\centering
\begin{tabular}{l l}
\textbf{Paket} & \textbf{Version} \\
\hline
Akka & 1.5.54 \\
Akka.Cluster & 1.5.54 \\
Akka.Cluster.Sharding & 1.5.54 \\
Akka.Hosting & 1.5.53 \\
Akka.Cluster.Hosting & 1.5.53 \\
Akka.Persistence & 1.5.54 \\
Akka.Persistence.Sql & 1.5.53 \\
Akka.Persistence.Hosting & 1.5.53 \\
Akka.Persistence.Sql.Hosting & 1.5.53 \\
Akka.Streams & 1.5.54 \\
Akka.Serialization.Hyperion & 1.5.54 \\
Npgsql & 9.0.4 \\
Serilog & 4.3.0 \\
\end{tabular}
\caption{Verwendete Kernbibliotheken und Versionen}
\label{tab:packages}
\end{table}

Die ausgewählten Pakete decken die zentralen technischen Anforderungen des Systems ab. Akka.Cluster und Akka.Cluster.Sharding 
ermöglichen eine verteilte und fehlertolerante Verarbeitung der eingehenden Daten. Akka.Persistence und die zugehörigen 
SQL-Erweiterungen stellen sicher, dass zustandsbehaftete Entitäten konsistent wiederhergestellt werden können. Akka.Streams 
wird für die kontrollierte Verarbeitung der Datenströme eingesetzt. Mit den Hosting-Erweiterungen wird eine einheitliche 
Konfiguration über das .NET-Hostingmodell ermöglicht, wodurch die Initialisierung des Clusters sowie der Persistence-Komponenten 
vereinfacht wird. Serilog und Npgsql ergänzen die Architektur durch Logging und Datenbankanbindung.


\section{Konfiguration und Initialisierung des Clusters}

\subsection{HOCON Konfiguration}

Die Konfiguration eines Akka.NET-Systems kann auf zwei unterschiedliche Arten erfolgen. 
Zum einen kann die HOCON-Konfiguration aus einer externen Datei eingelesen werden, die den vollständigen 
Konfigurationstext enthält. Zum anderen ermöglicht Akka.Hosting die Erstellung der Konfiguration direkt 
über Erweiterungsmethoden innerhalb des .NET-Hostingmodells.

Beide Ansätze wurden im Rahmen dieses Projekts eingesetzt. Für Konsolenanwendungen ist das Laden einer 
separaten HOCON-Datei zweckmäßig, da der Overhead des Hostingmodells dort nicht erforderlich ist. 
In Anwendungen, die auf dem .NET-Hostingmodell basieren, ist die programmgesteuerte Konfiguration über 
Akka.Hosting dagegen besonders vorteilhaft, da sie eine engere Integration in den Lebenszyklus des Hosts 
und eine klar strukturierte Initialisierung des Clusters ermöglicht.

\subsubsection*{HOCON File}

In klassischen Konsolenanwendungen ist es üblich, die Akka.NET-Konfiguration in einer separaten HOCON-Datei 
zu hinterlegen und beim Start der Anwendung einzulesen.

\begin{program}[H]
\caption{Auszug aus der HOCON-Konfiguration des Clients}
\label{prog:hocon-client}
\begin{GenericCode}
akka {
    loglevel = "INFO"
    stdout-loglevel = "OFF"
    loggers = ["Akka.Logger.Serilog.SerilogLogger, Akka.Logger.Serilog"]
    
    actor {
      provider = cluster
      serializers {
        hyperion = "Akka.Serialization.HyperionSerializer, Akka.Serialization.Hyperion"
      }
      serialization-bindings {
        "System.Object" = hyperion
      }
    }
    
    remote.dot-netty.tcp { 
        hostname = "localhost"
        port = 0 
        maximum-frame-size = 2MiB
        send-buffer-size    = 2MiB
        receive-buffer-size = 2MiB
    
    }
    
    cluster {
      roles = ["api"]
      seed-nodes = [
        "akka.tcp://cluster-system@localhost:5000"
      ]
    }
}
\end{GenericCode}
\end{program}

Wie in Programm~\ref{prog:hocon-client} dargestellt, folgt die Konfigurationsdatei der HOCON-Syntax, 
die strukturell an JSON erinnert, jedoch flexibler und für Akka.NET optimiert ist. 
Jede Akka-Konfiguration beginnt mit dem Hauptelement \texttt{akka}, unter dem die zentralen 
Systemparameter definiert werden.

Die initialen Einträge betreffen die Konfiguration des Loggings. Anschließend wird über 
\texttt{actor.provider = cluster} festgelegt, dass dieser Prozess Teil eines verteilten Clusters ist. 
Die Serialisierung der Nachrichten wird mithilfe des Hyperion-Serialisierers konfiguriert, der eine 
effiziente binäre Datenrepräsentation ermöglicht und sich dadurch besonders für Clusterkommunikation eignet.

Der Block \texttt{remote.dot-netty.tcp} enthält die Netzwerkkonfiguration. 
Der Port ist auf \texttt{0} gesetzt, wodurch das System zur Laufzeit automatisch einen freien Port auswählt. 
Weitere Parameter wie \texttt{maximum-frame-size} oder die Puffergrößen steuern die maximale Nachrichtengröße 
und die Netzwerkeffizienz der Punkt-zu-Punkt-Kommunikation.

Im Abschnitt \texttt{cluster} werden die Rollen des Knotens sowie die \textit{seed nodes} festgelegt. 
Seed-Knoten dienen als initiale Kontaktpunkte beim Beitritt eines neuen Knotens zum Cluster. 
Nach der ersten Verbindung erfolgt die weitere Clusterentdeckung über das Gossip-Protokoll. 
Die Rollendefinition \texttt{api} legt fest, welche Aufgaben und Aktoren diesem Knoten im Clusterverbund zugeordnet sind.

\subsubsection*{Akka.Hosting}

um nicht immer eine Hocon-Datei reinladen zu müssen, wo gegebenfalls leichter fehler entstehen können, wurde Akka.Hosting entwickelt, 
um die Konfiguration direkt im Code zu ermöglichen. Dies ist nützlich, da man so auch direkt in die Dependency Injection Pipeline des .NET Hostings integrieren kann.

Bei meinem Projekt ist es wie schon beschrieben möglich die anwendung entweder als Monolithen als auch als verteiltes System zu starten. Daher habe ich die Konfiguration
so wie in Abbildung~\ref{prog:hocon-hosting} gezeigt implementiert, um durch definierte Rollen die verschiedenen Komponenten des Systems zu initialisieren.
Dies geschiedt durch parameter eingabe in der Console, die dann in der AkkaConfig Klasse ausgewertet werden. 

Um die Konfiguration übersichtlich zu halten, habe ich Erweiterungsmethoden implementiert, die die Initialisierung der verschiedenen Rollen kapseln.
Anderen Code der bestimmte initialisierung vornimmt, wie z.B. Cluster Optionen und Remote sowie die Serialisierung und Logging Konfigurationen, wurden in Erweiterungsmethoden ausgelagert.


Anstatt die Konfiguration ausschließlich über externe HOCON-Dateien zu definieren, die beim manuellen Bearbeiten 
anfällig für Fehler sein können, bietet Akka.Hosting die Möglichkeit, die Akka.NET-Konfiguration direkt im 
Anwendungscode aufzubauen. Dadurch lässt sich die Konfiguration eng in die Dependency Injection Pipeline des 
.NET-Hostingmodells integrieren und gemeinsam mit anderen Diensten der Anwendung verwalten.

In diesem Projekt kann die Anwendung wahlweise als monolithischer Prozess oder als verteiltes System mit mehreren 
Clusterknoten ausgeführt werden. Die Konfiguration wurde daher so gestaltet, dass auf Basis der zur Laufzeit 
vergebenen Rollen die jeweils benötigten Komponenten initialisiert werden. Ein entsprechender Ausschnitt ist in 
Programm~\ref{prog:hocon-hosting} dargestellt. Die Rollen werden über Parameter der Konsolenanwendung festgelegt 
und anschließend in der Klasse \texttt{AkkaConfig} ausgewertet.

Um die Konfiguration übersichtlich und wartbar zu halten, wurden zentrale Initialisierungsschritte in 
Erweiterungsmethoden gekapselt. Diese Methoden übernehmen unter anderem die Einrichtung der Clusteroptionen, 
der Remote-Kommunikation sowie der Serialisierungs- und Loggingkonfiguration und binden die jeweiligen Rollen 
wie Ingress, Backend oder Coordinator in das Hostingmodell ein.


\begin{program}[H]
\caption{Auszug aus der Akka.Hosting-Konfiguration}
\label{prog:hocon-hosting}
\begin{CsCode}
public static IServiceCollection UseAkka(this IServiceCollection sp, AkkaConfig akkaHc, ConfigurationManager manager)
{
    string connectionString = manager.GetConnectionString("PostgreSql")!;

    sp.AddAkka(akkaHc.ClusterName, akka =>
    {
        
        akka.UseAkkaLogger();
        akka.UseRemoteCluster(akkaHc);
        akka.UseHyperion();

        if (akkaHc.Roles.Contains(ClusterMemberRoles.Controller.ToStr()))
            akka.RegisterCoordinator(akkaHc);

        if (akkaHc.Roles.Contains(ClusterMemberRoles.Backend.ToStr()))
        {
            akka.RegisterShardRegion(akkaHc);
            akka.RegisterBackendJournal(connectionString);
        }
            

        if (akkaHc.Roles.Contains(ClusterMemberRoles.Ingress.ToStr()))
            akka.RegisterIngress(akkaHc);

        if (!akkaHc.Roles.Contains(ClusterMemberRoles.Controller.ToStr()))
            akka.RegisterControllerProxy();
    });
    return sp;
}
\end{CsCode}
\end{program}

\subsubsection*{Akka.Hosting Koordinator-Initialisierung}

Um exemplarisch zu zeigen, wie die Initialisierung der verschiedenen Rollen erfolgt, ist in Programm~\ref{prog:hocon-hosting-coordinator} 
ein Auszug der Konfiguration für die \textit{Coordinator}-Rolle dargestellt.

Zunächst wird ein Singleton-Aktor des Typs \texttt{ClusterCoordinator} registriert, der zentrale Steuerungsaufgaben 
innerhalb des Clusters übernimmt. Damit andere Komponenten diesen Aktor über die Dependency-Injection-Pipeline 
referenzieren können, wird ein sogenannter \textit{Marker} (\texttt{ClusterCoordinatorMarker}) verwendet. 
Ein solcher Marker dient als eindeutiger Schlüssel, über den sich ein bestimmter Aktor später typsicher abrufen lässt.

Im Anschluss werden weitere Aktoren registriert, die für das Monitoring von Clusterereignissen sowie für die 
Verarbeitung von Ingress- und Shard-bezogenen Ereignissen zuständig sind. Auf diese Weise wird die Beobachtung 
und Verwaltung relevanter Clusterzustände dezentral über spezialisierte Listener-Aktoren umgesetzt.

Die Methode \texttt{resolver.Props<T>()} ist eine Erweiterung von Akka.Hosting und ermöglicht die Erstellung 
von Aktoren unter Nutzung der Dependency Injection. Mit \texttt{system.ActorOf(...)} wird der jeweilige Aktor 
schließlich instanziert und dem ActorSystem hinzugefügt.

Über \texttt{registry.Register<T>()} wird der erzeugte Aktor anschließend im internen Registry-System von 
Akka.Hosting hinterlegt, damit er später von anderen Diensten oder Modulen abgerufen werden kann.

\begin{program}[H]
\caption{Auszug aus der Akka.Hosting-Koordinator-Konfiguration}
\label{prog:hocon-hosting-coordinator}
\begin{CsCode}
private static void RegisterCoordinator(this AkkaConfigurationBuilder config, AkkaConfig akkaHc)
{
    string role = ClusterMemberRoles.Controller.ToStr();

    config.WithSingleton<ClusterCoordinatorMarker>(
            singletonName: role,
            propsFactory: (_, _, resolver) => resolver.Props<ClusterCoordinator>(),
            options: new ClusterSingletonOptions { Role = akkaHc.Role })
        .WithActors((system, registry, resolver) =>
        {
            var controller = registry.Get<ClusterCoordinatorMarker>();

            registry.Register<ClusterEventListener>(
                system.ActorOf(resolver.Props<ClusterEventListener>(),
                    "cluster-event-listener"));

            registry.Register<IngressListener>(
                system.ActorOf(resolver.Props<IngressListener>(controller),
                    "ingress-listener"));

            registry.Register<ShardListener>(
                system.ActorOf(resolver.Props<ShardListener>(controller),
                    "shard-listener"));
        });
}
\end{CsCode}
\end{program}

\subsubsection*{Akka.Hosting Backend-Initialisierung}

Die \textit{Backend}-Rolle stellt das Herzstück der verteilten Datenverarbeitung dar, da sie die
\texttt{ShardRegion} hostet, in der die einzelnen Fahrerdaten als zustandsbehaftete Entitäten
repräsentiert und verarbeitet werden. Programm~\ref{prog:hocon-hosting-backend} zeigt einen
ausgewählten Ausschnitt der Konfiguration dieser Rolle.

Für die ShardRegion wird die \textit{DistributedData}-Variante des Shardings verwendet.
Hierbei werden die internen Sharding-Metadaten, insbesondere die Zuordnung von Entitäten zu Shards
sowie der aktuelle Aktivierungszustand der Entitäten, über das Modul
\texttt{Akka.DistributedData} repliziert. Dies erhöht die Ausfallsicherheit, da bei einem Knotenverlust
die übrigen Clusterknoten weiterhin einen konsistenten Überblick über die Shard-Verteilung behalten.
Der eigentliche fachliche Zustand der Fahrerdaten wird hingegen durch die persistenten Entitätsaktoren
(\texttt{DriverActorPersistent}) verwaltet und über Akka.Persistence im Journal gespeichert.

Zu Beginn werden die Optionen für das verteilte Sharding konfiguriert. Parameter wie
\texttt{MajorityMinimumCapacity} legen fest, wie viele Clusterknoten für eine Mehrheitsentscheidung
innerhalb von DistributedData erforderlich sind, während \texttt{MaxDeltaElements} bestimmt, wie
umfangreich die über Gossip propagierten Delta-States sein dürfen. Diese Einstellungen beeinflussen das
Verhalten der Replikation insbesondere bei wachsender Anzahl von Entitäten.

Im Anschluss wird die eigentliche ShardRegion registriert. Ein zugehöriger \textit{Marker}
(\texttt{DriverRegionMarker}) ermöglicht den späteren Zugriff über die Dependency-Injection-Pipeline.
Die Entitäten innerhalb der ShardRegion werden als persistent implementierte Aktoren des Typs
\texttt{DriverActorPersistent} ausgeführt. Ein \texttt{MessageExtractor}
(\texttt{DriverMessageExtractor}) ordnet eingehende Nachrichten sowohl den Shards als auch den einzelnen
Entitäten zu und übernimmt damit eine zentrale Rolle in der Lastverteilung.

Zusätzlich werden Optionen wie die automatische Passivierung inaktiver Entitäten sowie die Nutzung des
DistributedData-Modus als Speichermechanismus für Sharding-Metadaten konfiguriert. Mit
\texttt{RememberEntities = true} wird sichergestellt, dass Entitäten nach einem Neustart oder einem
Rebalancing automatisch wieder aktiviert werden.

\begin{program}[H]
\caption{Auszug aus der Akka.Hosting-Backend-Konfiguration}
\label{prog:hocon-hosting-backend}
\begin{CsCode}
private static void RegisterShardRegion(this AkkaConfigurationBuilder config, AkkaConfig akkaHc, IMessageExtractor? ex = null)
{
    string role = ClusterMemberRoles.Backend.ToStr();

    var extractor = ex ?? new DriverMessageExtractor();

    config.WithShardingDistributedData(options =>
        {
            options.RecreateOnFailure = true;
            options.MajorityMinimumCapacity = 2;
            options.MaxDeltaElements = 2000;
            options.Durable.Keys = [];
        })
        .WithShardRegion<DriverRegionMarker>(
            typeName: akkaHc.ShardName,
            entityPropsFactory: (_, _, resolver) => _ => resolver.Props<DriverActorPersistent>(),
            messageExtractor: extractor,
            shardOptions: new ShardOptions
            {
                Role = role,
                PassivateIdleEntityAfter = TimeSpan.FromMinutes(5),
                StateStoreMode = StateStoreMode.DData,
                RememberEntities = true
            })
        .WithActors((system, registry, resolver) =>
            registry.Register<TelemetryRegionHandler>(
                system.ActorOf(resolver.Props<TelemetryRegionHandler>(),
                    "telemetry-region-handler")));
}
\end{CsCode}
\end{program}


\subsubsection*{Akka.Hosting Ingress-Initialisierung}

Die \textit{Ingress}-Rolle ist für den Empfang und die Vorverarbeitung der eingehenden Fahrerdaten
zuständig. Programm~\ref{prog:hocon-hosting-ingress} zeigt einen Ausschnitt der Konfiguration dieser Rolle.

Zunächst wird für die HTTP-Kommunikation mit dem Server ein Singleton-Aktor des Typs 
\texttt{IngressHttpActor} registriert. Dieser Aktor stellt beim Start eine Verbindung zum Cloud-Server her 
und übernimmt den Empfang der eingehenden Datenströme.

Anschließend wird ein ShardRegion-Proxy für die Kommunikation mit der \texttt{ShardRegion} erstellt. 
Hierzu wird ein Marker (\texttt{DriverRegionProxyMarker}) verwendet, über den der Proxy später über die 
Dependency-Injection-Pipeline referenziert werden kann. Der Proxy ermöglicht es den Ingress-Komponenten, 
Nachrichten an die tatsächliche ShardRegion zu senden, ohne direkt an deren physische Instanz gebunden zu sein. 
Dies fördert eine lose Kopplung zwischen Ingress und Backend.

Zusätzlich wird ein \texttt{IngressControllerActor} registriert, der als zentrale Steuerungskomponente 
für die Ingress-Logik dient und die weitere Verarbeitung der eingehenden Daten innerhalb des Aktorensystems koordiniert.

\begin{program}[H]
\caption{Auszug aus der Akka.Hosting-Ingress-Konfiguration}
\label{prog:hocon-hosting-ingress}
\begin{CsCode}
private static void RegisterIngress(this AkkaConfigurationBuilder config, AkkaConfig akkaHc)
{
    string role = ClusterMemberRoles.Ingress.ToStr();
    config
        .WithSingleton<HttpWrapperClientMarker>(
            singletonName: role,
            propsFactory: (_, _, resolver) => resolver.Props<IngressHttpActor>(),
            options: new ClusterSingletonOptions {Role = akkaHc.Role})
        .WithShardRegionProxy<DriverRegionProxyMarker>(
            typeName: akkaHc.ShardName,
            roleName: ClusterMemberRoles.Backend.ToStr(),
            messageExtractor: new DriverMessageExtractor())
        .WithActors((system, registry, resolver) =>
        {
            registry.Register<IngressControllerActor>(
                system.ActorOf(resolver.Props<IngressControllerActor>(),
                    "controller-handler"));
        });
}
\end{CsCode}
\end{program}

\subsubsection*{Akka.Hosting Generelle Remote und Cluster Konfiguration}

Damit ein AktorSystem als Clusterknoten mit anderen Systemen kommunizieren kann, müssen Remoting und 
Clustering konfiguriert werden. Programm~\ref{prog:hocon-hosting-remote} zeigt eine eigene 
Erweiterungsmethode, die diese Konfiguration bündelt und von allen Rollen gemeinsam genutzt wird. 

Zunächst werden Hostname und Port für das Remote-Modul gesetzt, sodass der Knoten über TCP erreichbar ist. 
Anschließend werden die Clusteroptionen konfiguriert. Dazu gehören die Rollen des Knotens sowie die Seed Nodes, 
über die der Cluster initial aufgebaut wird. Je nach Startkonfiguration wird entweder die vollständige Seed-Liste 
oder nur der erste Seed Node verwendet, um eine stabile Initialisierung zu gewährleisten.

Für die spätere Nachrichtenverteilung zwischen unabhängigen Aktoren wird außerdem der 
\textit{DistributedPubSub}-Mediator aktiviert. Dieser ermöglicht eine publish/subscribe-basierte Kommunikation, 
ohne dass Sender und Empfänger direkt voneinander wissen müssen. Dies fördert eine lose Kopplung innerhalb des Systems.

Abschließend werden Parameter wie die maximale Nachrichtengröße und die Puffergrößen 
für die TCP-Kommunikation definiert. Diese Einstellungen sind insbesondere bei der Übertragung größerer 
Datenmengen relevant und tragen zur Stabilität und Effizienz der Clusterkommunikation bei.

\begin{program}[H]
\caption{Auszug aus der Akka.Hosting-Remote-Cluster-Konfiguration}
\label{prog:hocon-hosting-remote}
\begin{CsCode}
public static AkkaConfigurationBuilder UseRemoteCluster(this AkkaConfigurationBuilder builder, AkkaConfig config)
{
    builder
        .WithRemoting(
            port: config.Port,
            hostname: config.Hostname)
        .WithClustering(
            new ClusterOptions
            {
                SeedNodes = config.Roles.Count == 1 ? config.SeedNodes : [config.SeedNodes.First()],
                Roles = config.Roles.ToArray()
            })
        .WithDistributedPubSub(role: null!)
        .AddHocon("""
                      akka.remote.dot-netty.tcp {
                        maximum-frame-size  = 2 MiB
                        send-buffer-size    = 2 MiB
                        receive-buffer-size = 2 MiB
                      }
                      """, HoconAddMode.Append);

    return builder;
}

\end{CsCode}
\end{program}

\subsubsection*{Akka.Hosting Persistenzkonfiguration}

Die Persistenz ist ein zentraler Bestandteil der Backend-Rolle, da der Zustand der Fahrerdaten dauerhaft 
gesichert werden muss.Programm~\ref{prog:hocon-hosting-journal} zeigt die Einbindung der 
Persistence-Komponenten in das Aktorensystem mithilfe von Akka.Hosting.

Die Implementierung unterstützt zwei Betriebsmodi.
Zum einen wird eine In-Memory-Persistenz für Testläufe verwendet, die ein schnelles und reproduzierbares 
Verhalten ohne externe Abhängigkeiten ermöglicht.Zum anderen kommt im produktiven Betrieb eine PostgreSQL-basierte 
Persistenz zum Einsatz. Dadurch kann der Zustand der Fahrerdaten auch über Neustarts hinweg konsistent wiederhergestellt werden.

Für den Datenbankzugriff wird Npgsql eingesetzt. Über \texttt{WithSqlPersistence} wird Akka.Persistence.Sql in das 
System integriert und automatisch mit den erforderlichen Journal- und Snapshot-Schemata initialisiert.
Die In-Memory-Variante wird automatisch aktiv, wenn keine gültige Datenbankverbindung konfiguriert wurde, 
wodurch die Backend-Rolle flexibel in unterschiedlichen Umgebungen betrieben werden kann.


\begin{program}[H]
\caption{Auszug aus der Akka.Hosting-Journal-Konfiguration}
\label{prog:hocon-hosting-journal}
\begin{CsCode}
private static void RegisterBackendJournal(this AkkaConfigurationBuilder config, string connectionString)
{
    string dbName = "driverRegion";

    if (connectionString is "")
    {
        config.WithInMemoryJournal(journalId: dbName, journalBuilder: _ => { });
        config.WithInMemorySnapshotStore(dbName);
        return;
    }
    
    var dataSource = new NpgsqlDataSourceBuilder(connectionString).Build();

    var dataOptions = new DataOptions()
        .UseDataProvider(
            DataConnection.GetDataProvider(
                ProviderName.PostgreSQL,
                dataSource.ConnectionString) ?? throw new Exception("Could not get data provider"))
        .UseProvider(ProviderName.PostgreSQL)
        .UseConnectionFactory(_ => dataSource.CreateConnection());

    config.WithSqlPersistence(dataOptions, autoInitialize: true, schemaName: "public");

    config.AddStartup((sys, ct) =>
    {
        var c = sys.Settings.Config;
        sys.Log.Info("Journal={0}, Snapshot={1}",
            c.GetString("akka.persistence.journal.plugin"),
            c.GetString("akka.persistence.snapshot-store.plugin"));

        sys.RegisterOnTermination(() => dataSource.Dispose());
        return Task.CompletedTask;
    });
}

\end{CsCode}
\end{program}

\section{Eingangs-Service}

Datenabgriff von OpenF1
Datenabgriff von OpenF1
Normalisierung der Daten
Übergabe an Streams
Retry-Strategien
Fehlerbehandlung

\section{Datenbearbeitung mit Akka.Streams}

Aufbau des Stream-Graphs
Flow → Map → Filter → Buffer → Sink
Backpressure im konkreten System
Overflowstrategien (Verweis auf Kapitel 3)
Materialization

\section{Verarbeitung in der ShardRegion}

Der \texttt{DriverActorPersistent} bildet die zentrale Komponente zur konsistenten Verwaltung des Zustands eines einzelnen 
Fahrers innerhalb einer \texttt{ShardRegion}.Durch die Ableitung von \texttt{ReceivePersistentActor} besitzt er die Fähigkeit, 
Zustandsänderungen als Ereignisse (Events) dauerhaft zu persistieren. Diese Persistenzmechanismen ermöglichen es dem Aktor, 
seinen vollständigen Zustand bei einem Neustart, einem Ausfall einzelner Clusterknoten oder einem Rebalancing des Clusters 
zuverlässig wiederherzustellen.

\subsection{Konstruktor und Wiederherstellung}

Zu Beginn der Aktorinitialisierung werden mithilfe der Methode \texttt{RecoverState} alle zuvor gespeicherten Snapshots sowie 
die nach dem letzten Snapshot aufgetretenen Ereignisse geladen. Dies gewährleistet, dass sich der Zustand des Fahrers auch 
nach einem Ausfall oder einer Migration innerhalb des Clusters vollständig rekonstruieren lässt.

Ein vorhandener \texttt{SnapshotOffer} stellt dabei den zuletzt gespeicherten Zustandsstand bereit, wohingegen alle 
nachfolgenden Ereignisse sequentiell erneut angewendet werden, um den Zustand auf den aktuellen Stand zu bringen.
Der Wiederherstellungsprozess endet mit dem Ereignis \texttt{RecoveryCompleted}. Anschließend wird geprüft, ob der Zustand 
bereits initialisiert wurde. In diesem Fall wechselt der Aktor in das Verhalten \texttt{Initialized}, andernfalls verbleibt 
er im Zustand \texttt{Uninitialized}.

\begin{program}[H]
\caption{Auszug aus dem DriverActorPersistent}
\begin{CsCode}
public DriverActorPersistent(IRequiredActor<TelemetryRegionHandler> handler)
{
    _handler = handler.ActorRef;
    _logger.Info(\$"DriverActorPersistent constructor: {Self.Path.Name}");
    RecoverState();

    Become(Uninitialized);
}

private void RecoverState()
{
    Recover<SnapshotOffer>(offer =>
    {
        _logger.Info(\$"SnapshotOffer for {offer.Snapshot}");
        if (offer.Snapshot is DriverInfoState state)
        {
            _state.RestoreFromSnapshot(state);
            _logger.Info(\$"Recovered snapshot for driver {_state.ToDriverInfoString()}");
        }
    });

    Recover<IHasDriverId>(evt =>
    {
        _state.Apply(evt);
        _logger.Info("Replayed {Event} for {Pid}", evt.GetType().Name, PersistenceId);
    });

    Recover<RecoveryCompleted>(_ =>
    {
        if (_state.IsInitialized) Become(Initialized);
    });
}

\end{CsCode}
\end{program}

\subsection{Initialisierung des Aktors}

Solange der Aktor noch nicht initialisiert wurde, befindet er sich im Verhalten \texttt{Uninitialized}.Dieser Zustand wird 
im Konstruktor über \texttt{Become(Uninitialized)} aktiviert. Die Methode \texttt{Become} ersetzt sämtliche bisher 
registrierten Nachrichtenhandler und ermöglicht damit ein dynamisches, zustandsabhängiges Verhalten des Aktors.

Erst wenn der Aktor eine \texttt{CreateModelDriverMessage} empfängt, wird diese persistiert, auf den internen Zustand 
angewendet und damit die Initialisierung abgeschlossen. Nach erfolgreicher Initialisierung wechselt der Aktor in das 
Verhalten \texttt{Initialized}, in dem er regulär auf weitere fachliche Nachrichten reagieren kann.

Empfängt der Aktor vor Abschluss der Initialisierung eine andere Nachricht, wird er passiviert, um unnötige 
Ressourcenbelegung zu vermeiden. Dies erfolgt über einen \texttt{CommandAny}-Handler, der nicht behandelte 
Nachrichten protokolliert und gleichzeitig die Passivierung der Shard-Entity anstößt.

\begin{program}[H]
\caption{Auszug aus dem DriverActorPersistent}
\begin{CsCode}
private void Uninitialized()
{
    Command<CreateModelDriverMessage>(m =>
    {
        Persist(m, evt =>
        {

            try
            {
                _state.Apply(evt);
                _logger.Info(\$"Initialized driver {_state.ToDriverInfoString()})");
                Sender.Tell(new Status.Success(CreatedDriverMessage.Success(_state.Key)));
                Become(Initialized);
            }
            catch (ArgumentNullException ex)
            {
                _logger.Error(ex, "Failed to initialize driver with message: {Message}", m);
                Sender.Tell(new Status.Failure(ex));
                Context.System.PubSub().Api.Publish(new NotifyStatusFailureMessage(ex.Message));
            }
        });
    });

    Command<StopEntity>(_ => Context.Stop(Self));

    CommandAny(msg =>
    {
        var entityId = Self.Path.Name;
        _logger.Warning(\$"Received {msg.GetType().Name} before initialization for entity {entityId}. Passivating.");

        Sender.Tell(new NotInitializedMessage(entityId));
        Context.Parent.Tell(new Passivate(new StopEntity()));
        Context.System.PubSub().Api.Publish(new NotifyStatusFailureMessage("DriverActor was not init"));
    });   
}
\end{CsCode}
\end{program}

\subsection{Initialisierter DriverActorPersistent}

Nach erfolgter Initialisierung wechselt der Aktor in das Verhalten \texttt{Initialized}. In diesem Zustand reagiert er 
auf verschiedene fachliche Nachrichten, insbesondere auf Änderungen im Fahrerzustand. Jede Änderung wird über die 
Methode \texttt{PersistAndApply} verarbeitet, die die empfangene Nachricht persistiert, auf den internen Zustand anwendet 
und anschließend die aktualisierten Informationen an den \texttt{TelemetryRegionHandler} weiterleitet.
Dies stellt sicher, dass alle nachgelagerten Komponenten jederzeit über den aktuellen Zustand des Fahrers verfügen.

\begin{program}[H]
\caption{Auszug aus dem DriverActorPersistent}
\begin{CsCode}
private void Initialized()
{
    Command<UpdateTelemetryMessage>(m =>
    {
        _logger.Debug("Telemetry: {Id} speed={Speed} t={Ts:o}", _state.Key, m.Speed, m.TimestampUtc);
        PersistAndApply(m);
    });

    // weitere Handler ausgelassen
}

private void PersistAndApply(IHasDriverId element) 
{
    if (!_state.IsInitialized || !KeysMatchOrFail(element.Key))
    {
        Sender.Tell(
            new Status.Failure(
                new DriverInShardNotFoundException(element.Key, \$"Key is not {_state.Key} or initialized")));

        Context.System.PubSub().Api.Publish(new NotifyStatusFailureMessage(\$"Key is not {_state.Key} or initialized"));
        return;
    }

    Persist(element, ev =>
    {
        _state.Apply(ev);
        SendToHandler();
        CreateSnapshot();
    });
    Sender.Tell(new Status.Success(element));
}
\end{CsCode}
\end{program}

\subsection{Persistierung des Zustands}

Damit der Wiederherstellungsprozess effizient bleibt, erzeugt der Aktor in regelmäßigen Abständen einen Snapshot seines 
aktuellen Zustands. Die Methode \texttt{CreateSnapshot} überprüft dazu, ob die aktuelle Sequenznummer ein Vielfaches von 
zehn ist. Nur in diesem Fall wird ein Snapshot erzeugt und gespeichert. Dieses Verfahren reduziert die Anzahl der beim 
Recovery zu verarbeitenden Ereignisse und führt damit zu einer schnelleren Wiederherstellung, ohne bei jeder einzelnen 
Zustandsänderung einen Snapshot zu erzeugen.

\begin{program}[H]
\caption{Auszug aus dem DriverActorPersistent}
\begin{CsCode}
private void CreateSnapshot()
{
    if (LastSequenceNr % 10 == 0)
    {
        _logger.Debug("Creating snapshot for {Pid} at seqNr {Seq}", PersistenceId, LastSequenceNr);
        SaveSnapshot(_state.CopyState());
    }
}
\end{CsCode}
\end{program}

\subsection{Shard-Zuordnung und Rebalancing}

Die Zuordnung eingehender Nachrichten zu den jeweiligen Shards und Entitäten erfolgt über den \texttt{DriverMessageExtractor}, 
der sowohl die Entitäts-ID als auch die Shard-ID aus einer Nachricht ableitet. Dies geschieht über den Standardmechanismus von 
\texttt{Akka.Cluster.Sharding}, der auf dem \texttt{HashCodeMessageExtractor} basiert. Die Shard-Zuordnung ergibt sich dabei 
aus einer Hashfunktion, die von Akka.NET bereitgestellt wird und eine gleichmäßige Verteilung der Entitäten über alle verfügbaren 
Clusterknoten anstrebt.

Eine eigene Rebalancing-Strategie wurde nicht implementiert, da die Standardstrategie von \texttt{Akka.Cluster.Sharding} bereits 
eine effiziente Lastverteilung sicherstellt. Sie verschiebt Shards automatisch, sobald neue Clusterknoten hinzukommen oder 
bestehende Knoten ausfallen, und ermöglicht somit eine robuste Wiederherstellung der Systembalance ohne zusätzlichen Implementierungsaufwand.

\subsection*{Zusammenfassung}

Der \texttt{DriverActorPersistent} kombiniert persistente Ereignisverarbeitung mit einem klar strukturierten, 
zustandsabhängigen Aktormodell innerhalb einer \texttt{ShardRegion}. Während des Recoverys werden sowohl Snapshots als 
auch alle nachfolgenden Ereignisse rekonstruiert, wodurch ein konsistenter Fahrerzustand auch nach Ausfällen oder 
Rebalancing gewährleistet wird. Durch das Wechseln zwischen den Zuständen \texttt{Uninitialized} und \texttt{Initialized} 
wird sichergestellt, dass nur vollständig initialisierte Entities fachliche Nachrichten verarbeiten. Alle Zustandsänderungen 
werden über \texttt{PersistAndApply} als Ereignisse gespeichert und optional mittels periodischer Snapshots ergänzt. Damit 
erfüllt der Aktor die Anforderungen an ein fehlertolerantes, verteiltes und deterministisch rekonstruierbares 
Fahrermanagement im Cluster.

\section{Nachrichtenverteilung im Cluster}

Um bei einer losen Kopplung Nachrichten zwischen Aktoren auszutauschen, wird der \texttt{DistributedPubSub}-Mediator verwendet. 
Dadurch können Aktoren Nachrichten versenden und empfangen, ohne direkte Referenzen aufeinander zu besitzen.

\subsection{Mediator: Nachrichtenempfänger}

Damit ein Aktor Nachrichten über den Mediator empfangen kann, muss er sich mit einem bestimmten Topic beim 
\texttt{DistributedPubSub}-Mediator registrieren. Zur Vereinheitlichung wurde hierfür eine Basisklasse 
implementiert, die die Anmeldung beim Mediator übernimmt. Alle Aktoren, die Nachrichten über ein Topic 
empfangen sollen, erben von dieser Basisklasse.

Während der Anmeldephase werden eingehende Nachrichten zunächst gestasht, bis alle erwarteten Bestätigungen 
(\texttt{SubscribeAck}) eingetroffen sind. Erst danach wird das eigentliche Verhalten des Aktors aktiviert 
und die gestashten Nachrichten werden verarbeitet. Um zusätzlich Gruppenkommunikation zu unterstützen, 
erfolgt neben der Anmeldung am Topic eine weitere Anmeldung mit einer generierten Gruppen-ID. Dadurch können 
mehrere Aktoren dieselben Nachrichten erhalten, wenn sie mit derselben Gruppen-ID registriert sind.

\begin{program}[H]
\caption{Auszug aus der Basisklasse für PubSub-Empfänger}
\begin{CsCode}
protected override void PreStart()
{
    _pubSubActorRef = DistributedPubSub.Get(Context.System).Mediator;
    if (_pubSubActorRef.IsNobody())
        throw new ActorNotFoundException("Mediator not connected!");

    _member = PubSubTypeMapping.ToMember(typeof(TTopic)) ?? PubSubMember.All;
    _logger.Info(\$"Mediator is trying to connected to topic {_member.ToStr()}");

    _pubSubActorRef.Tell(new Subscribe(_member.ToStr(), Self));
    _pubSubActorRef.Tell(new Subscribe(_member.ToStr(), Self, GenerateGroupId(_member)));
}
\end{CsCode}
\end{program}

Nach der Anmeldung wird auf die Bestätigung durch den Mediator gewartet. Solange diese Bestätigung noch nicht 
eingetroffen ist, werden alle Nachrichten gestasht. Sobald die erwartete Anzahl an \texttt{SubscribeAck}-Nachrichten 
empfangen wurde, werden alle gesammelten Nachrichten entstapelt und das eigentliche Verhalten des Aktors aktiviert.

\begin{program}[H]
\caption{Auszug aus der Basisklasse für PubSub-Empfänger}
\begin{CsCode}
private void HandleAckSub()
{
    Receive<SubscribeAck>(msg =>
    {
        _logger.Info(\$"Grab Ack for {msg.Subscribe.Topic} with group ({msg.Subscribe.Group})");

        Become(() =>
        {
            Activated();
            Stash.UnstashAll();
        });
    });

    ReceiveAny(_ => Stash.Stash());
}
\end{CsCode}
\end{program}

\subsection{Mediator: Nachrichtensender}

Zum Versenden von Nachrichten wird der \texttt{DistributedPubSub}-Mediator referenziert und über eine 
\texttt{Publish}-Nachricht ein bestimmtes Topic adressiert. 
Um sicherzustellen, dass Nachrichten stets mit einem konsistenten und korrekt definierten Topic gesendet werden, 
wurden mehrere typisierte Zugriffsmethoden definiert.

Diese Methoden (\texttt{Api}, \texttt{Backend}, \texttt{Ingress}, \texttt{Controller}) stellen jeweils einen 
vordefinierten logischen Kommunikationskanal bereit. 
Anstatt frei formulierte Topic-Strings zu verwenden, wird das jeweils zugehörige Topic intern über die 
Zuordnung des entsprechenden Mitglieds bestimmt und anschließend über den Mediator veröffentlicht.

Auf diese Weise entsteht eine string-freie und fehlerresistente Mechanik zur Nachrichtenverteilung, 
die sowohl eine lose Kopplung der Aktoren als auch eine einheitliche Struktur der verwendeten Topics im gesamten 
System sicherstellt.

Programm~\ref{prog:Erweiterungsmethode Mediator} zeigt exemplarisch die Verwendung dieser Methoden.

\begin{program}[H]
\caption{Auszug aus der Erweiterungsmethode für PubSub-Sender}
\label{prog:Erweiterungsmethode Mediator}
\begin{CsCode}
Context.PubSub().Api.Publish(new NotifyDriverStateMessage(msg.Key, msg.State));
Context.PubSub().Backend.Publish(new NotifyDriverStateMessage(msg.Key, msg.State));
\end{CsCode}
\end{program}

\section{Cluster-Steuerung und Fehlertoleranz}

\subsection{ClusterCoordinator}

\begin{program}[H]
\caption{Auszug aus dem ClusterCoordinator}
\begin{CsCode}
Receive<ShardConnectionUpdateMessage>(msg =>
{
    _hasShardRegion = msg.IsShardOnline;
    var con = _hasShardRegion ? "connected" : "disconnected";
    _logger.Debug(\$"Shard connection changed. Shard {con}");
    _ingressListener.Tell(new IngressConnectionCanActivated(_hasShardRegion));
});

ReceiveAsync<IngressConnectivityRequest>(async _ =>
{
    if (!_hasShardRegion)
    {
        var res = await _shardListener
            .Ask<ShardConnectionUpdateMessage>(ShardConnectionRequest.Instance);
        _hasShardRegion = res.IsShardOnline;
    }

    _logger.Debug(\$"Ingress activate request. Shard active: {_hasShardRegion}");
    Sender.Tell(new IngressConnectivityResponse(_hasShardRegion));
});
\end{CsCode}
\end{program}

\subsection{ShardListener: Verfügbarkeit der ShardRegion}

\begin{program}[H]
\caption{Auszug aus dem ShardListener}
\begin{CsCode}
Receive<IncreaseClusterMember>(msg =>
{
    Logger.Debug("Increase counter");
    _activeBackends.Add(msg.ClusterMemberRef);
    SendCountUpdate(new ShardConnectionUpdateMessage(_activeBackends.Count > 0));
});

Receive<DecreaseClusterMember>(msg =>
{
    Logger.Debug("Decrease counter");
    _activeBackends.Remove(msg.ClusterMemberRef);
    SendCountUpdate(new ShardConnectionUpdateMessage(_activeBackends.Count > 0));
});

Receive<ShardCountRequest>(_ =>
    Sender.Tell(new ShardCountResponse(_activeBackends.Count)));
\end{CsCode}
\end{program}

\subsection{IngressListener: Benachrichtigung der Ingress-Knoten}

\begin{program}[H]
\caption{Auszug aus dem IngressListener}
\begin{CsCode}
Receive<IngressConnectionCanActivated>(msg =>
{
    var isOnline = msg.IsShardOnline ? "online" : "offline";
    Logger.Debug(\$"Auto send to shard is {isOnline} and ingress will be notified!");
    Context.PubSub().Ingress.Publish(
        new NotifyIngressShardIsOnline(msg.IsShardOnline));
});

Receive<IncreaseClusterMember>(msg =>
{
    Logger.Debug("Increase counter");
    _activeIngress.Add(msg.ClusterMemberRef);
    SendCountUpdate(new IngressConnectionUpdateMessage(_activeIngress.Count > 0));
});
\end{CsCode}
\end{program}

\subsection{ClusterEventListener: Verarbeitung von Cluster-Events}



\begin{program}[H]
\caption{Auszug aus dem ClusterEventListener}
\begin{CsCode}
Receive<ClusterEvent.MemberUp>(u =>
{
    _logger.Info(\$"Cluster Member is Up {u}, {string.Join(',', u.Member.Roles)}");
    foreach (var s in u.Member.Roles)
        Broadcast(ClusterMemberExtension.Parse(s),
                  new IncreaseClusterMember(u.Member.Address));
});

Receive<ClusterEvent.MemberRemoved>(r =>
{
    _logger.Info(\$"Cluster Member is Removed {r}, {string.Join(',', r.Member.Roles)}");
    foreach (var s in r.Member.Roles)
        Broadcast(ClusterMemberExtension.Parse(s),
                  new DecreaseClusterMember(r.Member.Address));
});
\end{CsCode}
\end{program}



Hier erklärst du:
Coordinator-Singleton
Steuerung von ShardRegion-Verfügbarkeit
Handling von Node-Failures
Rebalancing-Verhalten (auf Implementationsebene)
Logs und Monitoring
Dieser Abschnitt zeigt deinem Professor, dass deine Architektur wirklich robust ist.

\section{Api oder Visualisierungsschicht}

Wenn du eine API oder UI hast, kannst du hier erklären:
Wie man auf Fahrer-Daten zugreift
Welche Endpunkte/Views es gibt
Wie Entity-Daten aggregiert dargestellt werden

\section{Zusammenführung der Komponenten}

Einfacher Abschnitt, um:
den Datenfluss von „OpenF1 → Streams → Sharding → PubSub“
End-to-End zu erklären
Das zeigt, dass du die Implementation im Ganzen verstehst.
